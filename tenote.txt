魏运慧

课程安排: Shell + Hadoop + Zookeeper + HA + Hive



Hadoop2.x 的核心组件: 
	HDFS
	Yarn
	MapReduce
	Common

HDFS的核心组件: 
	NameNode (元数据)
	DataNode (源数据)
	Secondary NameNode
Yarn的核心组件:
	ResourceManager 整个集群的老大
	NodeManager  单个节点的老大
	ApplicationMaster  负责数据切分、资源申请、容错监控等 
	Container  对资源的封装
MapReduce:
	Map阶段(分)
	Reduce阶段(合) 


准备虚拟机:
1. 克隆虚拟机
2. 修改 vim /etc/udev/rules.d/70-persistent-net.rules , 拷贝mac地址
   修改 vim /etc/sysconfig/network-scripts/ifcfg-eth0 , 修改mac地址以及IP地址
   修改 vim /etc/sysconfig/network  修改主机名
   修改 vim /etc/hosts ,配置 IP与主机名的映射.
   修改 Windows系统中 C:\Windows\System32\drivers\etc\hosts
	192.168.202.101 hadoop101
	192.168.202.102 hadoop102
	192.168.202.103 hadoop103
	192.168.202.104 hadoop104
	192.168.202.105 hadoop105
	192.168.202.106 hadoop106
	192.168.202.107 hadoop107
	192.168.202.108 hadoop108
	
3. 添加atguigu用户 :  
	useradd atguigu 
	passwd  atguigu
        vim /etc/sudoers : atguigu ALL=(ALL)       NOPASSWD:ALL 
	
4. 在/opt下创建 software   module 目录，并修改为 atguigu:atguigu




MR:
Job提交流程源码分析:
1. job.waitForCompletion(true); 
   提交job

2. 在waitForCompletion方法中 调用 submit();

3. 在submit方法中:
   3.1 ensureState(JobState.DEFINE); 确认Job的状态
   3.2 setUseNewAPI(); 设置使用新的API
   3.3 connect();
       [1]. 创建cluster对象 return new Cluster(getConfiguration())
       [2]. initialize 在该方法中创建cluster
            此处会判断是本地运行还是Yarn运行,最终会获取到不同的Runner.
   	    LocalJobRunner   YarnRunner
   3.4 submitJobInternal()
       [1].checkSpecs(job); 判断输出路径是否存在,如果存在抛出异常
       [2].JobSubmissionFiles.getStagingDir(cluster, conf);
           创建MR运行时用到的临时目录
	   例如:D:\tmp\hadoop-Administrator\mapred\staging\Administrator1400736982\.staging
       [3].submitClient.getNewJobID(); 生成一个Job的id
       [4].copyAndConfigureFiles(job, submitJobDir);
           拷贝并配置一些文件
       [5].writeSplits(job, submitJobDir); 生成切片信息，写到对应的位置
	   a. 默认使用的FileInputFormat是 TextInputFormat
	   b. input.getSplits(job); 获取切片信息

	      long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));  1 
	      long maxSize = getMaxSplitSize(job);  Long的最大值
	      本地的块大小为32M
	      获取切片大小: return Math.max(minSize, Math.min(maxSize, blockSize));

	      判断是否要继续切片:  bytesRemaining(剩余文件的长度)/splitSize > SPLIT_SLOP(1.1)


       [6].writeConf(conf, submitJobFile); 将配置信息写到job.xml
       [7].submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());
           最终提交Job
       [8].jtFs.delete(submitJobDir, true); 删除临时文件的内容


FileInputFormat:
	1. TextInputFormat  默认的,按照文件块的大小来切片.
	2. CombineTextInputFormat  解决小文件过多，导致生成过多的切片，进而导致过多的MapTask
	3. KeyValueTextInputFormat 通过指定的分隔符处理一行数据，生成k 和 v 传入Map阶段.
	                           不改变默认的切片规则，改变的是每行数据的处理方式
	4. NLineTextInputFormat   按照指定的N行，来对文件进行切片.
			          改变的是切片的规则，不改变每行数据的处理方式.
        5. 自定义FileInputFormat
	   5.1 继承FileInputFormat
	   5.2 重写 createRecordReader方法，选择性重写 isSplitable 、getSplits 等方法
	   5.3 自定义RecordReader 
	        [1]. 继承RecordReader
		[2]. 重写initialize  nextKeyValue getCurrentKey  getCurrentValue等方法


MapReduce工作流程:

MapTask:
1.  Job job = new Job(JobID.downgrade(jobid), jobSubmitDir);
    重新构建一个可以真正在LocalJobRunner中运行的Job.

2.  this.start();  真正执行的时候执行的是 LocalJobRunner中的Job中的run方法.

3.  runTasks(mapRunnables, mapService, "map"); 
      将所有的MapTask所对应的线程传入,准备执行每个MapTask的线程

4.  每个MapTask对应的线程类型是 LocalJobRunner$Job$MapTaskRunnable
    开启进入到每个线程对应的run方法.

5.  MapTask map = new MapTask(systemJobFile.toString(), mapId, taskId,info.getSplitIndex(), 1);
    在每个MapTask对应的线程中，创建出来具体的MapTask对象。

6.   map.run(localConf, Job.this); 执行MapTask对象的run方法.

7.   runNewMapper(job, splitMetaInfo, umbilical, reporter);
     在MapTask对象的run方法中，实际调用的是runNewMapper方法.

8.  mapper.run(mapperContext); 执行对应的xxxMapper的run方法. 实际上我们一般
    不会重写Mapper中的run方法，因此还是会调用Mapper中run方法.

9.   map(context.getCurrentKey(), context.getCurrentValue(), context);
     真正执行xxxMapper中的map方法.




Shuffle过程:
1. Mapper端的map方法之后, Recude端的reduce方法之前的过程，称之为Shuffle
2. map端调用context.write(K,V)写出以后，MR会通过MapOutPutBuffer将kv收集起来.
3. 在收集kv之前，需要先计算kv的分区号
4. 默认的缓冲区的大小为100M,阈值设置为0.8， 也就是当缓冲区的内容达到80%时， 就要开始
   往磁盘溢写.
5. 溢写之前需要先在内存中进行排序(快排)，排序通过key来比较，只排记录kv的index.不进行
   kv位置的交换.
6. 当最终数据全部都溢写到磁盘以后，可能会生成多个溢写文件, 还需要将生成的所有的溢写文件
   合并成一个大文件，且要有序,该过程使用的是归并排序.
7. 分区--> 收集 --> 排序--> 溢写(N次)-->合并-->等待Reduce来拉取数据.

Shuffle源码关键步骤:
1. 从MapTask的run方法， 进入到runNewMapper方法，
    根据设置的numReduceTasks的个数,决定获取到哪个OutputCollector
    if (job.getNumReduceTasks() == 0) {
      output = 
        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
    } else {
      output = new NewOutputCollector(taskContext, job, umbilical, reporter);
    }

2. 如果有Reduce阶段，上一步获取到的是OutputCollector。
   在 new NewOutputCollector(taskContext, job, umbilical, reporter)中，
   2.1  创建collector .   collector = createSortingCollector(job, reporter);
	在createSortingCollector 方法中，创建出MapOutputBuffer对象，并
	调用该对象的init方法。
	collector.init(context);
	[1].获取溢写百分比以及缓存区大小
	     
            job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);
            job.getInt(JobContext.IO_SORT_MB, 100);

	[2]. 获取排序器  ，实际获取到的是QuickSort
	    ReflectionUtils.newInstance(job.getClass("map.sort.class",
            QuickSort.class, IndexedSorter.class), job);
	
	[3]. 创建 kvBuffer用于存实际的KV, 创建kvMeta ,用于存kv的元数据,例如: index、partition、keystart、valStart


        [4]. 获取比较器 comparator = job.getOutputKeyComparator();	

	[5]. output counters  计数器

	[6]. compression  压缩

	[7]. combiner 合并

	[8]. 启动溢写线程
	      spillThread.setDaemon(true);
	      spillThread.start();
  
	[9]. 溢写




   2.2 获取分区数  partitions = jobContext.getNumReduceTasks();
	如果NumReduceTasks的数量为1 ,返回的分区号永远是0
	如果NumReduceTasks的数量大于1, 会默认使用HashPartitioner来返回分区号.


ReduceTask: